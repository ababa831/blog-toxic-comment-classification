{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ブログからコメントデータを簡単に取得する例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指定したブログの各エントリのコメント欄から，テキストデータを抽出する\n",
    "\n",
    "**仕組み**\n",
    "\n",
    "1. ブログコメントのソースURLを指定する．\n",
    "2. ブログコメント欄の，コメントテキストの先頭と終端の目印になるHTMLタグをソースコードから調査して，`start_comment`, `end_comment`変数に代入\n",
    "3. 両変数に挟まれた文字列をコメントテキストとして抽出．抽出したコメントは`comment_list`に追加される\n",
    "4. 3をコメント数分だけ実行\n",
    "5. 一秒間のsleepを挟む\n",
    "\n",
    "1-4を各エントリに対して実行する．一部エントリは消されているものもあるので，404エラーが出現したときは，次のエントリにパスするように例外処理する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e.g. xxxx.comというブログコメントを収集する場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import urllib\n",
    "comment_list=[]\n",
    "\n",
    "# 参照するエントリの範囲を指定(7509エントリまで存在している場合)\n",
    "article_idxes = np.arange(1, 7509)\n",
    "\n",
    "for article_idx in article_idxes:\n",
    "    # 各エントリのURL\n",
    "    src_url = \"http://xxxx.com/blog-entry-\" + str(article_idx) + \".html\"\n",
    "    # コメントテキストの先頭\n",
    "    start_comment = \"<strong></strong></p>\" \n",
    "    len_start = len(start_comment)\n",
    "    # コメントテキストの終端\n",
    "    end_comment = \"<div class=\\\"comment_navi\\\">\"\n",
    "    len_end = len(end_comment)\n",
    "\n",
    "    try:\n",
    "        response = urllib.request.urlopen(src_url)\n",
    "        content_b = response.read()\n",
    "        content_u = content_b.decode(\"utf-8\")\n",
    "    except HTTPError:\n",
    "        print(\"エントリ{:d}のコンテンツ取得時にHTTPError(404)発生\".format(article_idx))\n",
    "        pass\n",
    "    except :\n",
    "        print(\"エントリ{:d}のコンテンツ取得時にHTTPError以外のエラー発生\".format(article_idx))\n",
    "        pass\n",
    "    else:\n",
    "        offset = 0\n",
    "        is_terminated = 0\n",
    "        while is_terminated !=1 :\n",
    "            content_tmp = content_u[offset:]\n",
    "            start_head_idx = content_tmp.find(\"<strong></strong></p>\")\n",
    "            end_head_idx = content_tmp.find(\"<div class=\\\"comment_navi\\\">\")\n",
    "            if start_head_idx != -1:\n",
    "                comment = content_tmp[start_head_idx+len_start:end_head_idx].lstrip(\"\\r\\n\")\n",
    "                comment_list.append(comment)\n",
    "                # update offset for the next comment searching\n",
    "                offset += end_head_idx + len_end\n",
    "            else:\n",
    "                is_terminated = 1\n",
    "        sys.stdout.write(\"\\r エントリ{:d}のコメント抽出が完了\".format(article_idx))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 収集したコメントをcsvファイルとして保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_sample_df = pd.DataFrame(comment_list, columns=[\"comment_text\"])\n",
    "# ラベルを仮にすべて0として指定\n",
    "is_toxic = np.zeros(len(comment_list), dtype=\"uint8\")\n",
    "train_sample_df[\"is_toxic\"] = is_toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_df.to_csv(\"train_sample.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
